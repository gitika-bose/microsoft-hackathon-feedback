{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Feedback Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook \n",
    "1. Creates the classifier trained on User Voice and some manually labelled reddit data\n",
    "2. Applies that classifier to unfilitered reddit data \n",
    "3. Stores the useful feedback in a file called *reddit_powerpoint_feedback.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier: User Feedback vs. No Feedback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"positive_training_data.txt\", \"r\") as file_object:\n",
    "    x_pos = file_object.read()\n",
    "    x_pos = x_pos.split(\"\\n\")\n",
    "\n",
    "with open(\"negative_training_data.txt\", \"r\") as file_object:\n",
    "    x_neg = file_object.read()\n",
    "    x_neg = x_neg.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos = [word_tokenize(sen) for sen in x_pos]\n",
    "tokens_neg = [word_tokenize(sen) for sen in x_neg]\n",
    "\n",
    "lower_tokens_pos = [lower_token(token) for token in tokens_pos]\n",
    "lower_tokens_neg = [lower_token(token) for token in tokens_neg]\n",
    "\n",
    "filtered_words_pos = [removeStopWords(sen) for sen in lower_tokens_pos]\n",
    "filtered_words_neg = [removeStopWords(sen) for sen in lower_tokens_neg]\n",
    "\n",
    "X = filtered_words_pos + filtered_words_neg\n",
    "Y = [1]*len(filtered_words_pos) + [0]*len(filtered_words_neg)\n",
    "\n",
    "X = [' '.join(sen) for sen in X]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(X.toarray(), np.array(Y), test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Not feedback       0.63      0.86      0.73        28\n",
      "PPT User Feedback       0.91      0.75      0.82        56\n",
      "\n",
      "         accuracy                           0.79        84\n",
      "        macro avg       0.77      0.80      0.78        84\n",
      "     weighted avg       0.82      0.79      0.79        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, predictions, labels=[0,1], target_names=[\"Not feedback\", \"PPT User Feedback\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Reddit Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reddit_data(file_iteration):\n",
    "    filename = \"unfiltered_reddit_data\" + file_iteration + \".txt\"\n",
    "    with open(filename, \"r\") as file_object:\n",
    "        raw_data = file_object.read()\n",
    "        raw_data = raw_data.split(\"\\n\")\n",
    "    \n",
    "    data = [word_tokenize(sen) for sen in raw_data]\n",
    "    data = [lower_token(token) for token in data]\n",
    "    data = [removeStopWords(sen) for sen in data]\n",
    "    data = [' '.join(sen) for sen in data]\n",
    "    data = vectorizer.transform(data)\n",
    "    \n",
    "    predictions = clf.predict(data)\n",
    "    print(predictions)\n",
    "    \n",
    "    with open(\"reddit_user_feedbback.txt\", \"a\") as file_object:\n",
    "        for i in range(0, len(predictions)):\n",
    "            if predictions[i] == 1:\n",
    "                file_object.write(raw_data[i]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "filter_reddit_data(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
