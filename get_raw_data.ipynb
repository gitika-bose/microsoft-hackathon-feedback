{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Feedback Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been divided into two parts \n",
    "<br/><br/>\n",
    "(A) Training Data <br/>\n",
    "The model to classify reddit posts as user feedback was trained using positive examples from User Voice and negative examples labelled manually from a set of Reddit posts. The data has been stored in txt files *positive_training_data.txt* and *negative_training_data.txt* respectively. Running these code cells will allow you to update these files. This is only needed once in a while if you to add fresh data. Recommended: Add newly classified to improve the training as well.\n",
    "<br/><br/>\n",
    "(A) Social Media Raw Feedback <br/>\n",
    "Reddit contains a vast amount of user data, some of which is extremely valuable user feedback including requests for new features and improvements on existing ones. Hence, the model creates is applied on the bulk of reddit data to extract the relevant information. These cells extract unfiltered data from reddit given a query. I included two queries that can be used. This should also be done every couple of days to allow a constant influx of a data. In this case, the data is added to a new file instead of appending to the old data so as to not classify the same data multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Voice Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uservoice_page(page_number):\n",
    "    link = \"https://powerpoint.uservoice.com/forums/270149-powerpoint-for-the-web?filter=hot&page=\"+str(page_number)\n",
    "    html_uservoice = requests.get(link)\n",
    "    return html_uservoice\n",
    "\n",
    "def get_page_feedback(html_uservoice):\n",
    "    soup = BeautifulSoup(html_uservoice.content, 'html.parser')\n",
    "    feedback = soup.select('div[class=\"uvIdeaHeader\"]')\n",
    "    all_feedback = []\n",
    "    \n",
    "    for f in feedback:\n",
    "        text = f.get_text()\n",
    "        text = text.replace(\"\\n\", \"\").strip()\n",
    "        text = text.replace(\"    \", \". \").strip()\n",
    "        all_feedback.append(text)\n",
    "        \n",
    "    return all_feedback\n",
    "\n",
    "def get_uservoice_feedback():\n",
    "    with open(\"positive_training_data.txt\", \"a\") as file_object:\n",
    "        for i in range(1, 29):\n",
    "            html_uservoice = get_uservoice_page(i)\n",
    "            all_feedback = get_page_feedback(html_uservoice)\n",
    "            for f in all_feedback:\n",
    "                f = f+\"\\n\"\n",
    "                file_object.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_uservoice_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reddit Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this, copy go through the file to remove any positive examples and then copy over the remaining negative examples to negative_training_data.txt (and optionally delete this file)\n",
    "with open(\"unlabelled_training_data.txt\", \"a\") as file_object:\n",
    "    q_powerpoint = requests.get('https://www.reddit.com/r/powerpoint/search.json?q=powerpoint&sort=new&restrict_sr=0&limit=100', headers = {'User-agent': 'your bot 0.1'})\n",
    "    reddit_data = q_powerpoint.json()[\"data\"][\"children\"]\n",
    "    for r in reddit_data:\n",
    "        feedback = re.sub('\\[(.+)\\]\\(([^ ]+)( \"(.+)\")?\\)', '', r[\"data\"][\"title\"].replace(\"\\n\", \" \").strip()) + \" \" + re.sub('\\[(.+)\\]\\(([^ ]+)( \"(.+)\")?\\)', '', r[\"data\"][\"selftext\"].replace(\"\\n\", \" \").strip()) + \"\\n\"\n",
    "        if len(feedback) < 2200:\n",
    "            file_object.write(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unfiltered_reddit_data(query, file_iteration):\n",
    "    filename = \"unfiltered_reddit_data\" + file_iteration + \".txt\"\n",
    "    with open(filename, \"a\") as file_object:\n",
    "        q_powerpoint = requests.get(query, headers = {'User-agent': 'your bot 0.1'})\n",
    "        reddit_data = q_powerpoint.json()[\"data\"][\"children\"]\n",
    "        for r in reddit_data:\n",
    "            feedback = re.sub('\\[(.+)\\]\\(([^ ]+)( \"(.+)\")?\\)', '', r[\"data\"][\"title\"].replace(\"\\n\", \" \").strip()) + \" \" + re.sub('\\[(.+)\\]\\(([^ ]+)( \"(.+)\")?\\)', '', r[\"data\"][\"selftext\"].replace(\"\\n\", \" \").strip()) + \"\\n\"\n",
    "            if len(feedback) < 2200:\n",
    "                file_object.write(feedback)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run get_unfiltered_reddit_data\n",
    "# Query 1 (typically has better feature feedback): https://www.reddit.com/r/powerpoint/search.json?q=features&sort=new&restrict_sr=1&limit=100\n",
    "# Query 2: https://www.reddit.com/r/powerpoint/search.json?q=powerpoint&sort=new&restrict_sr=0&limit=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unfiltered_reddit_data(\"https://www.reddit.com/r/powerpoint/search.json?q=powerpoint&sort=new&restrict_sr=0&limit=100\", \"1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
